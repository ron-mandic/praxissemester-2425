<h1>Mistral API</h1>

<article>
	<a href="/test-1"><h2>Chat Completion</h2></a>
	<p>
		Hier lernen wir, wie man die API grundständig benutzt und wie das für einfache Chat Completions
		genutzt werden kann
	</p>
</article>

<article>
	<a href="/test-2"><h2>Chat Stream</h2></a>
	<p>
		Und wenn wir eine Echtzeitanwendung haben wollen, dann bauen wir die API so um, dass wir einen
		Stream erhalten. Die Antwort des Servers wird dann nicht nur einmalig, sondern immer wieder
		aktualisiert.
	</p>
</article>

<article>
	<a href="/test-3"><h2>Chat Completion with JSON</h2></a>
	<p>
		Und wenn die Developer ein bestimmtes Format benötigen, dann können wir das auch anbieten. Das
		geschieht, indem wir einfach im Anfragekörper mitgeben, dass die Ausgabe als JSON ankommen soll
	</p>
</article>

<article>
	<a href="/test-4"><h2>Embeddings</h2></a>
	<p>
		Und wenn wir die API noch weiter ausbauen wollen, dann können wir auch Embeddings anbieten. Das
		heißt, dass wir nicht nur die Antwort des Servers bekommen, sondern auch gleich die Antwort
		verarbeiten und darstellen können.
	</p>
</article>

<article>
	<a href="/test-5"><h2>Chat Completion + Embeddings</h2></a>
	<p>
		Jetzt können wir die generierten Embeddings auch gleich in die Chat Completions einbauen. Das
	</p>
</article>

<article>
	<a href="/test-6"><h2>Chat Completion + Embeddings + Supabase</h2></a>
	<p>
		Mit Supabase können wir die Datenbankanbindung auch gleich mit einbauen. Das heißt, dass wir
		unsere Embeddings hochladen und dann direkt in unserem Chat benutzen können
	</p>
</article>

<article>
	<a href="/test-7"><h2>Function calling</h2></a>
	<p>
		Wir können nicht nur kontextbezogene Informationen abfragen, sondern auch Funktionen aufrufenl
		lassen, die die LLM für uns bestimmt hat. So können auf Anforderungen des Users reagieren.
	</p>
</article>

<style lang="scss">
	h1 {
		margin: 20px 0 2rem;
	}
</style>
