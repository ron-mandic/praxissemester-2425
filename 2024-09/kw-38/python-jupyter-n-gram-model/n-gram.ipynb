{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lib.utils import load, preprocess, get_dict, ask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_LENGTH_MIN = 1 # uni-gram\n",
    "NGRAM_CONTEXT_WINDOW = 3\n",
    "\n",
    "END_TOKEN = \"</s>\"\n",
    "WITH_END_TOKEN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_str = load('./data/songs.json')\n",
    "del list_str[\"rows\"][49] # Shape of You\n",
    "del list_str[\"rows\"][50 - 1] # You Need Me, I Don't Need You\n",
    "del list_str[\"rows\"][70 - 2] # Dive\n",
    "del list_str[\"rows\"][76 - 3] # Take Me Back to London\n",
    "\n",
    "list_str = [preprocess(str[\"row\"][\"text\"]) for str in list_str[\"rows\"] if str[\"row\"][\"text\"]]\n",
    "\n",
    "if WITH_END_TOKEN:\n",
    "    # Treat new lines as a word or not\n",
    "    list_str = [str.replace(\"\\n\", f\" {END_TOKEN} \") for str in list_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'club', 'isnt', 'the', 'best', 'place', 'to', 'find', 'a', 'lover', '</s>', 'so', 'the', 'bar', 'is', 'where', 'i', 'go', '</s>', 'me', 'and', 'my', 'friends', 'at', 'the', 'table', 'doing', 'shots', '</s>', 'drinking', 'fast', 'and', 'then', 'we', 'talk', 'slow', '</s>', 'and', 'you', 'come', 'over', 'and', 'start', 'up', 'a', 'conversation', 'with', 'just', 'me', '</s>', 'and', 'trust', 'me', 'ill', 'give', 'it', 'a', 'chance', 'now', '</s>', 'take', 'my', 'hand', 'stop', 'put', 'van', 'the', 'man', 'on', 'the', 'jukebox', '</s>', 'and', 'then', 'we', 'start', 'to', 'dance', 'and', 'now', 'im', 'singing', 'like', '</s>', 'girl', 'you', 'know', 'i', 'want', 'your', 'love', '</s>', 'your', 'love', 'was', 'handmade', 'for', 'somebody', 'like', 'me', '</s>', 'come', 'on', 'now', 'follow', 'my', 'lead', '</s>', 'i', 'may', 'be', 'crazy', 'dont', 'mind', 'me', '</s>', 'say', 'boy', 'lets', 'not', 'talk', 'too', 'much', '</s>', 'grab', 'on', 'my', 'waist', 'and', 'put', 'that', 'body', 'on', 'me', '</s>', 'come', 'on', 'now', 'follow', 'my', 'lead', '</s>', 'come', 'come', 'on', 'now', 'follow', 'my', 'lead', '</s>', 'im', 'in', 'love', 'with', 'the', 'shape', 'of', 'you', '</s>', 'we', 'push', 'and', 'pull', 'like', 'a', 'magnet', 'do', '</s>', 'although', 'my', 'heart', 'is', 'falling', 'too', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'and', 'last', 'night', 'you', 'were', 'in', 'my', 'room', '</s>', 'and', 'now', 'my', 'bed', 'sheets', 'smell', 'like', 'you', '</s>', 'every', 'day', 'discovering', 'something', 'brand', 'new', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'oh', 'i', 'oh', 'i', 'oh', 'i', 'oh', 'i', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'oh', 'i', 'oh', 'i', 'oh', 'i', 'oh', 'i', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'oh', 'i', 'oh', 'i', 'oh', 'i', 'oh', 'i', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'every', 'day', 'discovering', 'something', 'brand', 'new', '</s>', 'im', 'in', 'love', 'with', 'the', 'shape', 'of', 'you', '</s>', 'one', 'week', 'in', 'we', 'let', 'the', 'story', 'begin', '</s>', 'were', 'going', 'out', 'on', 'our', 'first', 'date', '</s>', 'you', 'and', 'me', 'are', 'thrifty', 'so', 'go', 'all', 'you', 'can', 'eat', '</s>', 'fill', 'up', 'your', 'bag', 'and', 'i', 'fill', 'up', 'a', 'plate', '</s>', 'we', 'talk', 'for', 'hours', 'and', 'hours', 'about', 'the', 'sweet', 'and', 'the', 'sour', '</s>', 'and', 'how', 'your', 'family', 'is', 'doing', 'okay', '</s>', 'leave', 'and', 'get', 'in', 'a', 'taxi', 'then', 'kiss', 'in', 'the', 'backseat', '</s>', 'tell', 'the', 'driver', 'make', 'the', 'radio', 'play', 'and', 'im', 'singing', 'like', '</s>', 'girl', 'you', 'know', 'i', 'want', 'your', 'love', '</s>', 'your', 'love', 'was', 'handmade', 'for', 'somebody', 'like', 'me', '</s>', 'come', 'on', 'now', 'follow', 'my', 'lead', '</s>', 'i', 'may', 'be', 'crazy', 'dont', 'mind', 'me', '</s>', 'say', 'boy', 'lets', 'not', 'talk', 'too', 'much', '</s>', 'grab', 'on', 'my', 'waist', 'and', 'put', 'that', 'body', 'on', 'me', '</s>', 'come', 'on', 'now', 'follow', 'my', 'lead', '</s>', 'come', 'come', 'on', 'now', 'follow', 'my', 'lead', '</s>', 'im', 'in', 'love', 'with', 'the', 'shape', 'of', 'you', '</s>', 'we', 'push', 'and', 'pull', 'like', 'a', 'magnet', 'do', '</s>', 'although', 'my', 'heart', 'is', 'falling', 'too', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'and', 'last', 'night', 'you', 'were', 'in', 'my', 'room', '</s>', 'and', 'now', 'my', 'bed', 'sheets', 'smell', 'like', 'you', '</s>', 'every', 'day', 'discovering', 'something', 'brand', 'new', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'oh', 'i', 'oh', 'i', 'oh', 'i', 'oh', 'i', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'oh', 'i', 'oh', 'i', 'oh', 'i', 'oh', 'i', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'oh', 'i', 'oh', 'i', 'oh', 'i', 'oh', 'i', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'every', 'day', 'discovering', 'something', 'brand', 'new', '</s>', 'im', 'in', 'love', 'with', 'the', 'shape', 'of', 'you', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'im', 'in', 'love', 'with', 'the', 'shape', 'of', 'you', '</s>', 'we', 'push', 'and', 'pull', 'like', 'a', 'magnet', 'do', '</s>', 'although', 'my', 'heart', 'is', 'falling', 'too', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'last', 'night', 'you', 'were', 'in', 'my', 'room', '</s>', 'and', 'now', 'my', 'bed', 'sheets', 'smell', 'like', 'you', '</s>', 'every', 'day', 'discovering', 'something', 'brand', 'new', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'come', 'on', 'be', 'my', 'baby', 'come', 'on', '</s>', 'im', 'in', 'love', 'with', 'your', 'body', '</s>', 'every', 'day', 'discovering', 'something', 'brand', 'new', '</s>', 'im', 'in', 'love', 'with', 'the', 'shape', 'of', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Split the lines into words and lowercase them\n",
    "list_list_words = [re.split(r'[\\s\\n]+', str.lower().strip()) for str in list_str]\n",
    "list_str_comp = [\"+\".join(list_words) for list_words in list_list_words]\n",
    "\n",
    "print(list_list_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 1904\n",
      "1 182\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(list_words) for list_words in list_list_words]\n",
    "lengths_sorted = sorted(lengths)\n",
    "\n",
    "# The biggest n-gram can only be the length of the smallest song\n",
    "NGRAM_LENGTH_MAX = lengths_sorted[0]\n",
    "\n",
    "print(min(lengths), max(lengths))\n",
    "print(NGRAM_LENGTH_MIN, NGRAM_LENGTH_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i+dont+wanna', 51, 0.0010843911462652294, 0.29310344827586204),\n",
       " ('i+dont+need', 47, 0.0009993408602836428, 0.27011494252873564),\n",
       " ('i+dont+want', 22, 0.0004677765728987264, 0.12643678160919541),\n",
       " ('i+dont+care', 9, 0.00019136314345856987, 0.05172413793103448),\n",
       " ('i+dont+know', 8, 0.00017010057196317324, 0.04597701149425287),\n",
       " ('i+dont+love', 7, 0.00014883800046777657, 0.040229885057471264),\n",
       " ('i+dont+deserve', 4, 8.505028598158662e-05, 0.022988505747126436),\n",
       " ('i+dont+like', 4, 8.505028598158662e-05, 0.022988505747126436),\n",
       " ('i+dont+even', 4, 8.505028598158662e-05, 0.022988505747126436),\n",
       " ('i+dont+really', 4, 8.505028598158662e-05, 0.022988505747126436),\n",
       " ('i+dont+ever', 3, 6.378771448618996e-05, 0.017241379310344827),\n",
       " ('i+dont+but', 2, 4.252514299079331e-05, 0.011494252873563218),\n",
       " ('i+dont+reckon', 1, 2.1262571495396654e-05, 0.005747126436781609),\n",
       " ('i+dont+have', 1, 2.1262571495396654e-05, 0.005747126436781609),\n",
       " ('i+dont+</s>', 1, 2.1262571495396654e-05, 0.005747126436781609),\n",
       " ('i+dont+mess', 1, 2.1262571495396654e-05, 0.005747126436781609),\n",
       " ('i+dont+mix', 1, 2.1262571495396654e-05, 0.005747126436781609),\n",
       " ('i+dont+do', 1, 2.1262571495396654e-05, 0.005747126436781609),\n",
       " ('i+dont+understand', 1, 2.1262571495396654e-05, 0.005747126436781609),\n",
       " ('i+dont+get', 1, 2.1262571495396654e-05, 0.005747126436781609),\n",
       " ('i+dont+drink', 1, 2.1262571495396654e-05, 0.005747126436781609)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_key_tuple = get_dict(list_list_words, 3)\n",
    "list_key_tuple_answers = ask_dict(dict_key_tuple, \"i+dont\")\n",
    "dicts = {}\n",
    "\n",
    "list_key_tuple_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "\n",
      "the club isnt the best place to find a lover \n",
      "thought id find her in a bottle \n",
      "god make me another one \n",
      "you got the kind of look in your eyes \n",
      "well baby im just being honest uh \n",
      "and i am only being honest with you i \n",
      "i get lonely and make mistakes from time to time \n",
      "se enioma enko ye bibia be ye ye \n",
      "bibia be ye ye \n",
      "bibia be ye ye ye \n",
      "\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "def predict(list_list_words: list, input: str, seed = None, max_lines = 10, separator = \"+\"):\n",
    "    if len(input) == 0:\n",
    "        return \"Input cannot be empty\"\n",
    "    list_inputs = input.split(separator)\n",
    "    ngram_context_window = len(list_inputs) + 1\n",
    "    \n",
    "    # Provided that the input is sanitized already\n",
    "    arg = input\n",
    "    max_index = 0\n",
    "    generator = torch.Generator().manual_seed(seed) if seed else None\n",
    "\n",
    "    # Generate the dictionary\n",
    "    dict_key_tuple = dicts.get(ngram_context_window) or get_dict(list_list_words, ngram_context_window)\n",
    "    if ngram_context_window not in dicts:\n",
    "        dicts[ngram_context_window] = dict_key_tuple\n",
    "\n",
    "    # Print over all list_words as a starter\n",
    "    print(\"START\\n\")\n",
    "    for _input in list_inputs:\n",
    "        print(_input, end=\" \")\n",
    "\n",
    "    while True:\n",
    "        # Get normalized probabilities for that input\n",
    "        list_key_tuple_answers = ask_dict(dict_key_tuple, arg)\n",
    "        if len(list_key_tuple_answers) == 0: return None\n",
    "\n",
    "        # Extract probability tensor\n",
    "        tensor_probs = torch.tensor([rel for (_, _, _, rel) in list_key_tuple_answers], dtype=torch.float32)\n",
    "\n",
    "        # Sample through the tensor\n",
    "        dict_multinomial = {\n",
    "            \"input\": tensor_probs,\n",
    "            \"num_samples\": 1,\n",
    "            \"replacement\": True\n",
    "        }\n",
    "        if generator:\n",
    "            dict_multinomial[\"generator\"] = generator\n",
    "        i = torch.multinomial(**dict_multinomial).item()\n",
    "\n",
    "        # Get the key using the sampled index\n",
    "        key = list_key_tuple_answers[i][0]\n",
    "\n",
    "        # Split the key at the separator (\"+\")\n",
    "        list_keys = key.split(separator)\n",
    "\n",
    "        # Iterate list_keys as long as there is no end token (</s>)\n",
    "        for _key in list_keys[len(list_keys) - 1:]:\n",
    "            if _key == END_TOKEN:\n",
    "                max_index += 1\n",
    "                print(\"\")\n",
    "            else:\n",
    "                print(_key, end=\" \")\n",
    "\n",
    "        # arg is the key replaced by the former arg and the separator\n",
    "        arg = \"+\".join(list_keys[1:])\n",
    "        if (max_index >= max_lines):\n",
    "            print(\"\\nEND\")\n",
    "            break\n",
    "\n",
    "predict(list_list_words, \"the+club+isnt\")\n",
    "\n",
    "# Possible questions:\n",
    "# - Can you generate the same output with both wihtout and with the seed?\n",
    "# - How to generate accurate song lyrics from Shape of You by Ed Sheeran?\n",
    "# - Why does your generation stop abruptly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n-gram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
